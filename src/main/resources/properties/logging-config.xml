<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="60 seconds" debug="false">
    <contextName>logback</contextName>
    <property name="log.path" value="./log"/>

    <!-- 输出到控制台的配置 -->
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 把控制台的内容输出到日志 -->
    <appender name="CONSOLE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>log/log-%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>90</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazon输出到独立文件的配置 -->
    <appender name="AMAZON" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有INFO及以上级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/amazon/amazon.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 数据库数据清洗输出到独立文件的配置 -->
    <appender name="DB_CLEAN_DATA" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/cleanData/dataBaseCleanData.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- douban定时爬虫任务配置 -->
    <appender name="DOUBAN_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/douban.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazon定时爬虫任务配置 -->
    <appender name="AMAZON_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/amazon.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- douban定时爬虫任务执行时间日志配置 -->
    <appender name="DOUBAN_CRAWL_STATUS" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/douban-status.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazon定时爬虫任务执行时间日志配置 -->
    <appender name="AMAZON_CRAWL_STATUS" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/amazon-status.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 针对AmazonMetaController配置的日志 -->
    <logger name="com.apabi.flow.douban.controller.AmazonMetaController" level="INFO" additivity="false">
        <!-- 对于AmazonMetaController的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于AmazonMetaController的输出至日志文件的日志配置 -->
        <appender-ref ref="AMAZON"/>
    </logger>

    <!-- 针对CleanDataController配置的日志 -->
    <logger name="com.apabi.flow.cleanData.controller.CleanDataController" level="INFO" additivity="false">
        <!-- 对于CleanDataController的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="DB_CLEAN_DATA"/>
    </logger>

    <!-- 针对douban定时爬虫任务配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.douban.DoubanConsumer" level="INFO" additivity="false">
        <!-- 对于douban定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="DOUBAN_CRAWL"/>
    </logger>

    <!-- 针对amazon定时爬虫任务配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.amazon.AmazonConsumer" level="INFO" additivity="false">
        <!-- 对于amazon定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="AMAZON_CRAWL"/>
    </logger>

    <!-- 针对douban定时爬虫任务执行时间配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.douban.CrawlDoubanService" level="INFO" additivity="false">
        <!-- 对于douban定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="DOUBAN_CRAWL_STATUS"/>
    </logger>

    <!-- 针对amazon定时爬虫任务执行时间配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.amazon.CrawlAmazonService" level="INFO" additivity="false">
        <!-- 对于amazon定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="AMAZON_CRAWL_STATUS"/>
    </logger>

    <root level="INFO">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>