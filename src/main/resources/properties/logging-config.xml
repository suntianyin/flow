<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="60 seconds" debug="false">
    <contextName>logback</contextName>
    <property name="log.path" value="./log"/>

    <!-- 输出到控制台的配置 -->
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 把控制台的内容输出到日志 -->
    <appender name="CONSOLE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>log/log-%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>90</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 流式分页接口输出到独立文件的配置 -->
    <appender name="PAGE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有INFO及以上级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/fetchPage/fetchPage.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 流式分页接口输出到独立文件的配置1 -->
    <appender name="PAGEAGAIN" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有INFO及以上级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/fetchPage1/fetchPageAgain.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 流式分页接口输出到独立文件的配置2 -->
    <appender name="PAGE2" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有INFO及以上级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/fetchPage2/fetchPage2.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazonAPI接口输出到独立文件的配置 -->
    <appender name="AMAZON" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有INFO及以上级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/amazon/amazon.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 数据库数据清洗输出到独立文件的配置 -->
    <appender name="DB_CLEAN_DATA" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/cleanData/dataBaseCleanData.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- douban定时爬虫任务配置 -->
    <appender name="DOUBAN_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/douban.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazon定时爬虫任务配置 -->
    <appender name="AMAZON_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/amazon.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- nlc定时爬虫任务配置 -->
    <appender name="NLC_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/nlc.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- nlc分类爬虫任务执行时间日志配置 -->
    <appender name="NLC_CATEGORY_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/nlc-category.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- douban定时爬虫任务执行时间日志配置 -->
    <appender name="DOUBAN_CRAWL_STATUS" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/douban-status.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazon定时爬虫任务执行时间日志配置 -->
    <appender name="AMAZON_CRAWL_STATUS" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/amazon-status.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- amazon定时爬虫任务执行时间日志配置 -->
    <appender name="DANGDANG_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/dangdang.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- nlc定时爬虫任务执行时间日志配置 -->
    <appender name="NLC_CRAWL_STATUS" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/nlc-status.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 京东定时抓取爬虫任务日志配置 -->
    <appender name="JD_CRAWL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/jd.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 京东全局抓取爬虫任务日志配置 -->
    <appender name="JD_CRAWL_ALL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/crawlTask/jd-all.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 针对FetchPageAgainConsumer接口配置的日志 -->
    <logger name="com.apabi.flow.book.fetchPage.FetchPageAgainConsumer" level="INFO" additivity="false">
        <!-- 对于FetchPageAgainConsumer的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于FetchPageAgainConsumer的输出至日志文件的日志配置 -->
        <appender-ref ref="PAGEAGAIN"/>
    </logger>

    <!-- 针对FetchPageConsumer接口配置的日志 -->
    <logger name="com.apabi.flow.book.fetchPage.FetchPageConsumer" level="INFO" additivity="false">
        <!-- 对于FetchPageConsumer的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于FetchPageConsumer的输出至日志文件的日志配置 -->
        <appender-ref ref="PAGE"/>
    </logger>


    <!-- 针对BookPageServiceImpl接口配置的日志 -->
    <logger name="com.apabi.flow.book.service.impl.BookPageServiceImpl" level="INFO" additivity="false">
        <!-- 对于BookPageServiceImpl的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于BookPageServiceImpl的输出至日志文件的日志配置 -->
        <appender-ref ref="PAGE2"/>
    </logger>
    <!-- 针对AmazonMetaController接口配置的日志 -->
    <logger name="com.apabi.flow.douban.controller.AmazonMetaController" level="INFO" additivity="false">
        <!-- 对于AmazonMetaController的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于AmazonMetaController的输出至日志文件的日志配置 -->
        <appender-ref ref="AMAZON"/>
    </logger>

    <!-- 针对CleanDataController配置的日志 -->
    <logger name="com.apabi.flow.cleanData.controller.CleanDataController" level="INFO" additivity="false">
        <!-- 对于CleanDataController的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="DB_CLEAN_DATA"/>
    </logger>

    <!-- 针对douban定时爬虫任务配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.douban.DoubanConsumer" level="INFO" additivity="false">
        <!-- 对于douban定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于DoubanConsumer的输出至日志文件的日志配置 -->
        <appender-ref ref="DOUBAN_CRAWL"/>
    </logger>

    <!-- 针对amazon定时爬虫任务配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.amazon.AmazonConsumer" level="INFO" additivity="false">
        <!-- 对于amazon定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于AmazonConsumer的输出至日志文件的日志配置 -->
        <appender-ref ref="AMAZON_CRAWL"/>
    </logger>

    <!-- 针对nlc定时爬虫任务配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.nlc.NlcMarcConsumer" level="INFO" additivity="false">
        <!-- 对于nlc定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于NlcMarcConsumer的输出至日志文件的日志配置 -->
        <appender-ref ref="NLC_CRAWL"/>
    </logger>

    <!-- 针对nlc分类爬虫任务配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.nlc_category.category.NlcBookMarcCategoryConsumer" level="INFO" additivity="false">
        <!-- 对于nlc分类爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于NlcBookMarcCategoryConsumer的输出至日志文件的日志配置 -->
        <appender-ref ref="NLC_CATEGORY_CRAWL"/>
    </logger>

    <!-- 针对douban定时爬虫任务执行时间配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.douban.CrawlDoubanService" level="INFO" additivity="false">
        <!-- 对于douban定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="DOUBAN_CRAWL_STATUS"/>
    </logger>

    <!-- 针对amazon定时爬虫任务执行时间配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.amazon.AmazonIdProducer" level="INFO" additivity="false">
        <!-- 对于amazon定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于CleanDataController的输出至日志文件的日志配置 -->
        <appender-ref ref="AMAZON_CRAWL_STATUS"/>
    </logger>

    <!-- 针对nlc爬虫任务执行时间配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.nlc.CrawlNlcMarcService" level="INFO" additivity="false">
        <!-- 对于nlc定时爬虫任务的STDOUT的日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于nlc的输出至日志文件的日志配置 -->
        <appender-ref ref="NLC_CRAWL_STATUS"/>
    </logger>

    <!-- 针对jd定时抓取配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.jd.JdConsumer" level="INFO" additivity="false">
        <!-- 打印至控制台 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于Jd定时任务输出至日志文件的日志配置 -->
        <appender-ref ref="JD_CRAWL"/>
    </logger>

    <!-- 针对jd全局抓取配置的日志 -->
    <logger name="com.apabi.flow.jd.service.JdMetadataConsumer" level="INFO" additivity="false">
        <!-- 打印至控制台 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于Jd全局抓取输出至日志文件的日志配置 -->
        <appender-ref ref="JD_CRAWL_ALL"/>
    </logger>

    <!-- 针对dangdang定时任务抓取配置的日志 -->
    <logger name="com.apabi.flow.crawlTask.dangdang.DangdangConsumer" level="INFO" additivity="false">
        <!-- 对于JD全局抓取日志配置 -->
        <appender-ref ref="STDOUT"/>
        <!-- 对于dangdang定时任务输出至日志文件的日志配置 -->
        <appender-ref ref="DANGDANG_CRAWL"/>
    </logger>


    <!--从书苑获取图书数据，新增到流式图书库中-->
    <appender name="GET_BOOK_SHUYUAN" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- onMatch=ACCEPT,onMismatch=DENY:只有ERROR级别的日志才会被添加到日志中 -->
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/shuyuan/%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>
    <logger name="com.apabi.flow.book.task.GetBook4ShuyuanTask" level="INFO" additivity="false">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="GET_BOOK_SHUYUAN"/>
    </logger>


    <root level="INFO">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>